
# Topic modeling    

A topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body.  A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. 


## Latent Dirichlet Allocation 

> Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.  

The LDA model is guided by two principles: 

- **Each document is a mixture of topics**. In a 3 topic model we could assert that a document is 70% about topic A, 30 about topic B, and 0% about topic C.    

- **Every topic is a mixture of words**. A topic is considered a probabilistic distribution over multiple words.

```{r, echo = FALSE, out.width = "120%", fig.cap = "Source: http://nlpx.net/wp/wp-content/uploads/2016/01/LDA_image2.jpg"}
knitr::include_graphics("images/LDA.jpg")
```

In particular, LDA is a imagined generative process, illustrated in the plate notation below:  


```{r, echo = FALSE, out.width = "120%", fig.cap = "(ref:LDA)"}
knitr::include_graphics("images/LDA_GPM.png")
```


(ref:LDA) Source: @lee  


- $M$ denotes the number of documents   
- $N$ is the number of words in a given document (document $i$ has $N_i$ words)   
- $\vec{\theta_m}$ is the expected topic proportion of document $m$, which is generated by a Dirichlet distribution parameterized by $\vec{\alpha}$ (e.g., in a two topic model $\theta_m = [0.3, 0.7]$ means document $m$ is expected to have 30% topic 1 and 70% topic 2)    
- $\vec{\phi_k}$ is the word distribution of topic $k$, which is generated by a Dirichlet distribution parameterized by $\vec{\beta}$  
- $z_{m, n}$ is the topic for the $n$th word in document $m$, one word are assigned to one topic.  
- $w_{m, n}$ is the word in the $n$th position word of document $m$   

The only observed variable in this graphical probabilistic model is $w_{m, n}$, so it is "latent".

To actually infer the topics in a corpus, we imagine the generative process as follows. LDA assumes the following generative process for a corpus $D$ consisting of $M$ M documents each of length $N_i$:  

1. Generate $\vec{\theta_i} \sim \text{Dir}(\vec{\alpha})$, where $i \in \{1, 2, ..., M\}$. $\text{Dir}(\vec{\alpha})$ is a Dirichlet distribution with symmetric parameter $\vec{\alpha}$ where $\vec{\alpha}$ is often sparse.

2. Generate $\vec{\phi_k} \sim \text{Dir}(\vec{\beta})$, where $k \in \{1, 2, ..., K\}$ and $\vec{\beta}$ is typically sparse  

3. For the $n$th position in document $m$, where $n \in \{1, 2, ..., N_m\}$ and $m \in \{1, 2, ..., M\}$   
    a. Choose a topic $z_{m, n}$ for that position which is generated from $z_{m, n} \sim \text{Multinomial}(\vec{\theta_i})$   
    b. Fill in that position with word $w_{m, n}$ which is generated from the word distribution of the topic picked in the previous step $w_{i,j} \sim \text{Multinomial}(\phi_{z_{m, n}})$ 
  
### Example: Associated Press  

We come to the `AssociatedPress` document term matrix (the required data strcture for the modeling function) and fit a two topic LDA model with `topicmodels::LDA()`  

```{r}
library(topicmodels)
data("AssociatedPress")

ap_lda <- LDA(AssociatedPress, k = 2)
ap_lda
```

For tidying model objects, `tidy(model_object, matrix = "beta")` (the default) access the topic-word probability vector (we denotes with $\vec{\phi_k}$)  

```{r}
tidy(ap_lda)
```

Which words have a relateve higher probabiltity to appear in each topic?    

```{r}
tidy(ap_lda) %>% 
  group_by(topic) %>%
  top_n(10) %>% 
  ggplot(aes(y = reorder_within(term, beta, topic), x = beta)) + 
  geom_col(aes(fill = factor(topic)), show.legend = FALSE) + 
  scale_y_reordered() + 
  facet_wrap(~ topic, scales = "free") + 
  labs(y = "",
       title = "Words with highest probability in each topic")
```

As an alternative, we could consider the terms that had the **greatest difference** in $\vec{\phi_k}$ between topic 1 and topic 2. This can be estimated based on the log ratio of the two: $\log_2(\frac{\phi_{1n}}{\phi_{2n}})$, $\phi_{1n} / \phi_{2n}$ being the probability ratio of the sam e word $n$ in two topics (a log ratio is useful because it makes the difference symmetrical)  

```{r}
phi_ratio <- tidy(ap_lda) %>%
  mutate(topic = str_c("topic", topic)) %>% 
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))
```



This can answer a question like: which word is most representative of a topic?  

```{r}
phi_ratio %>% 
  top_n(20, abs(log_ratio)) %>% 
  ggplot(aes(y = fct_reorder(term, log_ratio),
             x = log_ratio)) + 
  geom_col() + 
  labs(y = "",
       x = "log ratio of phi between topic 2 and topic 1 (base 2)")
```


To extrac the word proportion vector $\vec{\theta_m}$ for document $m$, use `matrix = "gamma"` in `tidy()`  

```{r}
tidy(ap_lda, matrix = "gamma")
```


With this data frame, we want to knwo which document is most charateristic of each topic?  

```{r}
library(reshape2)
library(wordcloud)

tidy(ap_lda, matrix = "gamma") %>% 
  group_by(topic) %>%
  top_n(15) %>% 
  mutate(document = as.character(document)) %>% 
  acast(document ~ topic, value.var = "gamma", fill = 0) %>% 
  comparison.cloud(colors = c("gray20", "gray80"), scale = c(2, 8))
```

This plot would definitely be more insightful if we have document titles rather than an ID.  

## Example: the great library heist  

To evaluate our topic model, we first divided 4 books into chapters. If a topic model with $K = 4$ performs well, then there should be a corresponding segmentation among those chpaters coming from those 4 different books.  

```{r}
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")  

library(gutenbergr)

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title")
```

```{r}

# add a chapter column
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, regex("^chapter ", ignore_case = TRUE)))) %>%
  ungroup() %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)


# find document-word counts
word_counts <- by_chapter %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>%
  rename(chapter = document) %>% 
  count(chapter, word, sort = TRUE) %>%
  ungroup()

word_counts
```


```{r}
books_lda <- word_counts %>% 
  cast_dtm(document = chapter, term = word, value = n) %>%
  LDA(k  = 4)


```

