<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.1 Tokenizing by n-gram | Notes for “Text Mining with R: A Tidy Approach”</title>
  <meta name="description" content="4.1 Tokenizing by n-gram | Notes for “Text Mining with R: A Tidy Approach”" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4.1 Tokenizing by n-gram | Notes for “Text Mining with R: A Tidy Approach”" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="enixam/tidy-text-mining" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.1 Tokenizing by n-gram | Notes for “Text Mining with R: A Tidy Approach”" />
  
  
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-07-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="relationships-between-words-n-grams-and-correlations.html"/>
<link rel="next" href="counting-and-correlating-pairs-of-words-with-widyr.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.4.0/str_view.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes for Text Mining with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="part"><span><b>I Text Mining with R</b></span></li>
<li class="chapter" data-level="1" data-path="tidy-text-format.html"><a href="tidy-text-format.html"><i class="fa fa-check"></i><b>1</b> Tidy text format</a><ul>
<li class="chapter" data-level="1.1" data-path="the-unnest-tokens-function.html"><a href="the-unnest-tokens-function.html"><i class="fa fa-check"></i><b>1.1</b> The <code>unnest_tokens()</code> function</a></li>
<li class="chapter" data-level="1.2" data-path="the-gutenbergr-package.html"><a href="the-gutenbergr-package.html"><i class="fa fa-check"></i><b>1.2</b> The <code>gutenbergr</code> package</a></li>
<li class="chapter" data-level="1.3" data-path="compare-word-frequency.html"><a href="compare-word-frequency.html"><i class="fa fa-check"></i><b>1.3</b> Compare word frequency</a></li>
<li class="chapter" data-level="1.4" data-path="other-tokenization-methods.html"><a href="other-tokenization-methods.html"><i class="fa fa-check"></i><b>1.4</b> Other tokenization methods</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="sentiment-analysis-with-tidy-data.html"><a href="sentiment-analysis-with-tidy-data.html"><i class="fa fa-check"></i><b>2</b> Sentiment analysis with tidy data</a><ul>
<li class="chapter" data-level="2.1" data-path="the-sentiments-dataset.html"><a href="the-sentiments-dataset.html"><i class="fa fa-check"></i><b>2.1</b> The <code>sentiments</code> dataset</a></li>
<li class="chapter" data-level="2.2" data-path="sentiment-analysis-with-inner-join.html"><a href="sentiment-analysis-with-inner-join.html"><i class="fa fa-check"></i><b>2.2</b> Sentiment analysis with inner join</a></li>
<li class="chapter" data-level="2.3" data-path="comparing-3-different-dictionaries.html"><a href="comparing-3-different-dictionaries.html"><i class="fa fa-check"></i><b>2.3</b> Comparing 3 different dictionaries</a></li>
<li class="chapter" data-level="2.4" data-path="most-common-positive-and-negative-words.html"><a href="most-common-positive-and-negative-words.html"><i class="fa fa-check"></i><b>2.4</b> Most common positive and negative words</a></li>
<li class="chapter" data-level="2.5" data-path="wordclouds.html"><a href="wordclouds.html"><i class="fa fa-check"></i><b>2.5</b> Wordclouds</a></li>
<li class="chapter" data-level="2.6" data-path="units-other-than-words.html"><a href="units-other-than-words.html"><i class="fa fa-check"></i><b>2.6</b> Units other than words</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analyzing-word-and-document-frequency.html"><a href="analyzing-word-and-document-frequency.html"><i class="fa fa-check"></i><b>3</b> Analyzing word and document frequency</a><ul>
<li class="chapter" data-level="3.1" data-path="tf-idf.html"><a href="tf-idf.html"><i class="fa fa-check"></i><b>3.1</b> tf-idf</a><ul>
<li class="chapter" data-level="3.1.1" data-path="tf-idf.html"><a href="tf-idf.html#term-frequency-in-jane-austens-novels"><i class="fa fa-check"></i><b>3.1.1</b> Term frequency in Jane Austen’s novels</a></li>
<li class="chapter" data-level="3.1.2" data-path="tf-idf.html"><a href="tf-idf.html#zipfs-law"><i class="fa fa-check"></i><b>3.1.2</b> Zipf’s law</a></li>
<li class="chapter" data-level="3.1.3" data-path="tf-idf.html"><a href="tf-idf.html#word-rank-slope-chart"><i class="fa fa-check"></i><b>3.1.3</b> Word rank slope chart</a></li>
<li class="chapter" data-level="3.1.4" data-path="tf-idf.html"><a href="tf-idf.html#the-bind_tf_idf-function"><i class="fa fa-check"></i><b>3.1.4</b> The <code>bind_tf_idf()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html"><i class="fa fa-check"></i><b>3.2</b> Weighted log odds ratio</a><ul>
<li class="chapter" data-level="3.2.1" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#log-odds-ratio"><i class="fa fa-check"></i><b>3.2.1</b> Log odds ratio</a></li>
<li class="chapter" data-level="3.2.2" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#model-based-approach-weighted-log-odds-ratio"><i class="fa fa-check"></i><b>3.2.2</b> Model-based approach: Weighted log odds ratio</a></li>
<li class="chapter" data-level="3.2.3" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#discussions"><i class="fa fa-check"></i><b>3.2.3</b> Discussions</a></li>
<li class="chapter" data-level="3.2.4" data-path="weighted-log-odds-ratio.html"><a href="weighted-log-odds-ratio.html#bind_log_odds"><i class="fa fa-check"></i><b>3.2.4</b> <code>bind_log_odds()</code></a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="a-corpus-of-physics-texts.html"><a href="a-corpus-of-physics-texts.html"><i class="fa fa-check"></i><b>3.3</b> A corpus of physics texts</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="relationships-between-words-n-grams-and-correlations.html"><a href="relationships-between-words-n-grams-and-correlations.html"><i class="fa fa-check"></i><b>4</b> Relationships between words: n-grams and correlations</a><ul>
<li class="chapter" data-level="4.1" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html"><i class="fa fa-check"></i><b>4.1</b> Tokenizing by n-gram</a><ul>
<li class="chapter" data-level="4.1.1" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#filtering-n-grams"><i class="fa fa-check"></i><b>4.1.1</b> Filtering n-grams</a></li>
<li class="chapter" data-level="4.1.2" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#analyzing-bigrams"><i class="fa fa-check"></i><b>4.1.2</b> Analyzing bigrams</a></li>
<li class="chapter" data-level="4.1.3" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#using-bigrams-to-provide-context-in-sentiment-analysis"><i class="fa fa-check"></i><b>4.1.3</b> Using bigrams to provide context in sentiment analysis</a></li>
<li class="chapter" data-level="4.1.4" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#visualizing-a-network-of-bigrams-with-ggraph"><i class="fa fa-check"></i><b>4.1.4</b> Visualizing a network of bigrams with <code>ggraph</code></a></li>
<li class="chapter" data-level="4.1.5" data-path="tokenizing-by-n-gram.html"><a href="tokenizing-by-n-gram.html#visualizing-friends"><i class="fa fa-check"></i><b>4.1.5</b> Visualizing “friends”</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html"><i class="fa fa-check"></i><b>4.2</b> Counting and correlating pairs of words with <code>widyr</code></a><ul>
<li class="chapter" data-level="4.2.1" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html#counting-and-correlating-among-sections"><i class="fa fa-check"></i><b>4.2.1</b> Counting and correlating among sections</a></li>
<li class="chapter" data-level="4.2.2" data-path="counting-and-correlating-pairs-of-words-with-widyr.html"><a href="counting-and-correlating-pairs-of-words-with-widyr.html#pairwise-correlation"><i class="fa fa-check"></i><b>4.2.2</b> Pairwise correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="converting-to-and-from-non-tidy-formats.html"><a href="converting-to-and-from-non-tidy-formats.html"><i class="fa fa-check"></i><b>5</b> Converting to and from non-tidy formats</a><ul>
<li class="chapter" data-level="5.1" data-path="tidying-a-document-term-matrix.html"><a href="tidying-a-document-term-matrix.html"><i class="fa fa-check"></i><b>5.1</b> Tidying a document-term matrix</a></li>
<li class="chapter" data-level="5.2" data-path="casting-tidy-text-data-into-a-matrix.html"><a href="casting-tidy-text-data-into-a-matrix.html"><i class="fa fa-check"></i><b>5.2</b> Casting tidy text data into a matrix</a></li>
<li class="chapter" data-level="5.3" data-path="tidying-corpus-objects-with-metadata.html"><a href="tidying-corpus-objects-with-metadata.html"><i class="fa fa-check"></i><b>5.3</b> Tidying corpus objects with metadata</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="topic-modeling.html"><a href="topic-modeling.html"><i class="fa fa-check"></i><b>6</b> Topic modeling</a><ul>
<li class="chapter" data-level="6.1" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html"><i class="fa fa-check"></i><b>6.1</b> Latent Dirichlet Allocation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="latent-dirichlet-allocation.html"><a href="latent-dirichlet-allocation.html#example-associated-press"><i class="fa fa-check"></i><b>6.1.1</b> Example: Associated Press</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html"><i class="fa fa-check"></i><b>6.2</b> Example: the great library heist</a><ul>
<li class="chapter" data-level="6.2.1" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#lda-on-chapters"><i class="fa fa-check"></i><b>6.2.1</b> LDA on chapters</a></li>
<li class="chapter" data-level="6.2.2" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#per-document-classification"><i class="fa fa-check"></i><b>6.2.2</b> Per-document classification</a></li>
<li class="chapter" data-level="6.2.3" data-path="example-the-great-library-heist.html"><a href="example-the-great-library-heist.html#by-word-assignments-augment"><i class="fa fa-check"></i><b>6.2.3</b> By word assignments: <code>augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="tuning-number-of-topics.html"><a href="tuning-number-of-topics.html"><i class="fa fa-check"></i><b>6.3</b> Tuning number of topics</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="text-classification.html"><a href="text-classification.html"><i class="fa fa-check"></i><b>7</b> Text classification</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="reviews-on-regular-expressions.html"><a href="reviews-on-regular-expressions.html"><i class="fa fa-check"></i><b>A</b> Reviews on regular expressions</a><ul>
<li class="chapter" data-level="A.1" data-path="metacharacters-and-posix-character-classes.html"><a href="metacharacters-and-posix-character-classes.html"><i class="fa fa-check"></i><b>A.1</b> Metacharacters and POSIX character classes</a></li>
<li class="chapter" data-level="A.2" data-path="unicode-code-points-categories-blocks-and-scripts.html"><a href="unicode-code-points-categories-blocks-and-scripts.html"><i class="fa fa-check"></i><b>A.2</b> Unicode Code Points, Categories, Blocks, and Scripts</a><ul>
<li class="chapter" data-level="A.2.1" data-path="unicode-code-points-categories-blocks-and-scripts.html"><a href="unicode-code-points-categories-blocks-and-scripts.html#unicode-categories"><i class="fa fa-check"></i><b>A.2.1</b> Unicode categories</a></li>
<li class="chapter" data-level="A.2.2" data-path="unicode-code-points-categories-blocks-and-scripts.html"><a href="unicode-code-points-categories-blocks-and-scripts.html#unicode-scripts"><i class="fa fa-check"></i><b>A.2.2</b> Unicode scripts</a></li>
<li class="chapter" data-level="A.2.3" data-path="unicode-code-points-categories-blocks-and-scripts.html"><a href="unicode-code-points-categories-blocks-and-scripts.html#unicode-blocks"><i class="fa fa-check"></i><b>A.2.3</b> Unicode blocks</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="greedy-and-lazy-quantifiers.html"><a href="greedy-and-lazy-quantifiers.html"><i class="fa fa-check"></i><b>A.3</b> Greedy and lazy quantifiers</a></li>
<li class="chapter" data-level="A.4" data-path="looking-ahead-and-back.html"><a href="looking-ahead-and-back.html"><i class="fa fa-check"></i><b>A.4</b> Looking ahead and back</a></li>
<li class="chapter" data-level="A.5" data-path="backreferences.html"><a href="backreferences.html"><i class="fa fa-check"></i><b>A.5</b> Backreferences</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="text-processing-examples-in-r.html"><a href="text-processing-examples-in-r.html"><i class="fa fa-check"></i><b>B</b> Text processing examples in R</a><ul>
<li class="chapter" data-level="B.1" data-path="replacing-and-removing.html"><a href="replacing-and-removing.html"><i class="fa fa-check"></i><b>B.1</b> Replacing and removing</a></li>
<li class="chapter" data-level="B.2" data-path="combining-and-splitting.html"><a href="combining-and-splitting.html"><i class="fa fa-check"></i><b>B.2</b> Combining and splitting</a></li>
<li class="chapter" data-level="B.3" data-path="extracting-text-from-pdf-and-other-files.html"><a href="extracting-text-from-pdf-and-other-files.html"><i class="fa fa-check"></i><b>B.3</b> Extracting text from pdf and other files</a><ul>
<li class="chapter" data-level="B.3.1" data-path="extracting-text-from-pdf-and-other-files.html"><a href="extracting-text-from-pdf-and-other-files.html#office-documents"><i class="fa fa-check"></i><b>B.3.1</b> Office documents</a></li>
<li class="chapter" data-level="B.3.2" data-path="extracting-text-from-pdf-and-other-files.html"><a href="extracting-text-from-pdf-and-other-files.html#images"><i class="fa fa-check"></i><b>B.3.2</b> Images</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Written with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for “Text Mining with R: A Tidy Approach”</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tokenizing-by-n-gram" class="section level2">
<h2><span class="header-section-number">4.1</span> Tokenizing by n-gram</h2>
<p><code>unnest_tokens()</code> have been used to tokenize the text by word, or sometimes by sentence, which is useful for the kinds of sentiment and frequency analyses. But we can also use the function to tokenize into consecutive sequences of words of length <code>n</code>, called <strong>n-grams</strong>.</p>
<p>We do this by adding the <code>token = "ngrams"</code> option to unnest_tokens(), and setting n to the number of words we wish to capture in each n-gram. When we set <code>n</code>to 2, we are examining pairs of two consecutive words, often called “bigrams”:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="tokenizing-by-n-gram.html#cb66-1"></a><span class="kw">library</span>(janeaustenr)</span>
<span id="cb66-2"><a href="tokenizing-by-n-gram.html#cb66-2"></a></span>
<span id="cb66-3"><a href="tokenizing-by-n-gram.html#cb66-3"></a>austen_bigrams &lt;-<span class="st"> </span><span class="kw">austen_books</span>() <span class="op">%&gt;%</span></span>
<span id="cb66-4"><a href="tokenizing-by-n-gram.html#cb66-4"></a><span class="st">  </span><span class="kw">unnest_tokens</span>(bigram, text, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>)</span>
<span id="cb66-5"><a href="tokenizing-by-n-gram.html#cb66-5"></a></span>
<span id="cb66-6"><a href="tokenizing-by-n-gram.html#cb66-6"></a>austen_bigrams <span class="op">%&gt;%</span></span>
<span id="cb66-7"><a href="tokenizing-by-n-gram.html#cb66-7"></a><span class="st">  </span><span class="kw">count</span>(bigram, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</span>
<span id="cb66-8"><a href="tokenizing-by-n-gram.html#cb66-8"></a><span class="co">#&gt; # A tibble: 211,236 x 2</span></span>
<span id="cb66-9"><a href="tokenizing-by-n-gram.html#cb66-9"></a><span class="co">#&gt;   bigram      n</span></span>
<span id="cb66-10"><a href="tokenizing-by-n-gram.html#cb66-10"></a><span class="co">#&gt;   &lt;chr&gt;   &lt;int&gt;</span></span>
<span id="cb66-11"><a href="tokenizing-by-n-gram.html#cb66-11"></a><span class="co">#&gt; 1 of the   3017</span></span>
<span id="cb66-12"><a href="tokenizing-by-n-gram.html#cb66-12"></a><span class="co">#&gt; 2 to be    2787</span></span>
<span id="cb66-13"><a href="tokenizing-by-n-gram.html#cb66-13"></a><span class="co">#&gt; 3 in the   2368</span></span>
<span id="cb66-14"><a href="tokenizing-by-n-gram.html#cb66-14"></a><span class="co">#&gt; 4 it was   1781</span></span>
<span id="cb66-15"><a href="tokenizing-by-n-gram.html#cb66-15"></a><span class="co">#&gt; 5 i am     1545</span></span>
<span id="cb66-16"><a href="tokenizing-by-n-gram.html#cb66-16"></a><span class="co">#&gt; 6 she had  1472</span></span>
<span id="cb66-17"><a href="tokenizing-by-n-gram.html#cb66-17"></a><span class="co">#&gt; # ... with 2.112e+05 more rows</span></span></code></pre></div>
<div id="filtering-n-grams" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Filtering n-grams</h3>
<p>As one might expect, a lot of the most common bigrams are pairs of common (uninteresting) words, such as of the and to be: what we call “stop-words” (see Chapter 1). This is a useful time to use <code>tidyr::separate()</code>, which splits a column into multiple based on a delimiter. This lets us separate it into two columns, filter out stop words separately, and then combine the results.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="tokenizing-by-n-gram.html#cb67-1"></a>austen_separated &lt;-<span class="st"> </span>austen_bigrams <span class="op">%&gt;%</span><span class="st">  </span></span>
<span id="cb67-2"><a href="tokenizing-by-n-gram.html#cb67-2"></a><span class="st">  </span><span class="kw">separate</span>(bigram, <span class="dt">into =</span> <span class="kw">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word2&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span>
<span id="cb67-3"><a href="tokenizing-by-n-gram.html#cb67-3"></a></span>
<span id="cb67-4"><a href="tokenizing-by-n-gram.html#cb67-4"></a></span>
<span id="cb67-5"><a href="tokenizing-by-n-gram.html#cb67-5"></a>austen_united &lt;-<span class="st"> </span>austen_separated <span class="op">%&gt;%</span></span>
<span id="cb67-6"><a href="tokenizing-by-n-gram.html#cb67-6"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word1 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word,</span>
<span id="cb67-7"><a href="tokenizing-by-n-gram.html#cb67-7"></a>         <span class="op">!</span>word2 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span></span>
<span id="cb67-8"><a href="tokenizing-by-n-gram.html#cb67-8"></a><span class="st">  </span><span class="kw">unite</span>(bigram, <span class="kw">c</span>(word1, word2), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span>
<span id="cb67-9"><a href="tokenizing-by-n-gram.html#cb67-9"></a></span>
<span id="cb67-10"><a href="tokenizing-by-n-gram.html#cb67-10"></a>austen_united <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(bigram, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</span>
<span id="cb67-11"><a href="tokenizing-by-n-gram.html#cb67-11"></a><span class="co">#&gt; # A tibble: 33,421 x 2</span></span>
<span id="cb67-12"><a href="tokenizing-by-n-gram.html#cb67-12"></a><span class="co">#&gt;   bigram                n</span></span>
<span id="cb67-13"><a href="tokenizing-by-n-gram.html#cb67-13"></a><span class="co">#&gt;   &lt;chr&gt;             &lt;int&gt;</span></span>
<span id="cb67-14"><a href="tokenizing-by-n-gram.html#cb67-14"></a><span class="co">#&gt; 1 sir thomas          287</span></span>
<span id="cb67-15"><a href="tokenizing-by-n-gram.html#cb67-15"></a><span class="co">#&gt; 2 miss crawford       215</span></span>
<span id="cb67-16"><a href="tokenizing-by-n-gram.html#cb67-16"></a><span class="co">#&gt; 3 captain wentworth   170</span></span>
<span id="cb67-17"><a href="tokenizing-by-n-gram.html#cb67-17"></a><span class="co">#&gt; 4 miss woodhouse      162</span></span>
<span id="cb67-18"><a href="tokenizing-by-n-gram.html#cb67-18"></a><span class="co">#&gt; 5 frank churchill     132</span></span>
<span id="cb67-19"><a href="tokenizing-by-n-gram.html#cb67-19"></a><span class="co">#&gt; 6 lady russell        118</span></span>
<span id="cb67-20"><a href="tokenizing-by-n-gram.html#cb67-20"></a><span class="co">#&gt; # ... with 3.342e+04 more rows</span></span></code></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="tokenizing-by-n-gram.html#cb68-1"></a>austen_bigrams &lt;-<span class="st"> </span>austen_bigrams <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb68-2"><a href="tokenizing-by-n-gram.html#cb68-2"></a><span class="st">  </span><span class="kw">separate</span>(bigram, <span class="dt">into =</span> <span class="kw">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word2&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb68-3"><a href="tokenizing-by-n-gram.html#cb68-3"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word1 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span></span>
<span id="cb68-4"><a href="tokenizing-by-n-gram.html#cb68-4"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word2 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb68-5"><a href="tokenizing-by-n-gram.html#cb68-5"></a><span class="st">  </span><span class="kw">unite</span>(bigram, <span class="kw">c</span>(word1, word2), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)</span>
<span id="cb68-6"><a href="tokenizing-by-n-gram.html#cb68-6"></a></span>
<span id="cb68-7"><a href="tokenizing-by-n-gram.html#cb68-7"></a>austen_bigrams</span>
<span id="cb68-8"><a href="tokenizing-by-n-gram.html#cb68-8"></a><span class="co">#&gt; # A tibble: 44,784 x 2</span></span>
<span id="cb68-9"><a href="tokenizing-by-n-gram.html#cb68-9"></a><span class="co">#&gt;   book                bigram                  </span></span>
<span id="cb68-10"><a href="tokenizing-by-n-gram.html#cb68-10"></a><span class="co">#&gt;   &lt;fct&gt;               &lt;chr&gt;                   </span></span>
<span id="cb68-11"><a href="tokenizing-by-n-gram.html#cb68-11"></a><span class="co">#&gt; 1 Sense &amp; Sensibility jane austen             </span></span>
<span id="cb68-12"><a href="tokenizing-by-n-gram.html#cb68-12"></a><span class="co">#&gt; 2 Sense &amp; Sensibility austen 1811             </span></span>
<span id="cb68-13"><a href="tokenizing-by-n-gram.html#cb68-13"></a><span class="co">#&gt; 3 Sense &amp; Sensibility 1811 chapter            </span></span>
<span id="cb68-14"><a href="tokenizing-by-n-gram.html#cb68-14"></a><span class="co">#&gt; 4 Sense &amp; Sensibility chapter 1               </span></span>
<span id="cb68-15"><a href="tokenizing-by-n-gram.html#cb68-15"></a><span class="co">#&gt; 5 Sense &amp; Sensibility norland park            </span></span>
<span id="cb68-16"><a href="tokenizing-by-n-gram.html#cb68-16"></a><span class="co">#&gt; 6 Sense &amp; Sensibility surrounding acquaintance</span></span>
<span id="cb68-17"><a href="tokenizing-by-n-gram.html#cb68-17"></a><span class="co">#&gt; # ... with 4.478e+04 more rows</span></span></code></pre></div>
</div>
<div id="analyzing-bigrams" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Analyzing bigrams</h3>
<p>The result of separating bigrams is helpful for exploratory analyses of the text. As a simple example, we might be interested in the most common “streets” mentioned in each book:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="tokenizing-by-n-gram.html#cb69-1"></a>austen_bigrams <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb69-2"><a href="tokenizing-by-n-gram.html#cb69-2"></a><span class="st">  </span><span class="kw">separate</span>(bigram, <span class="dt">into =</span> <span class="kw">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word2&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>)  <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb69-3"><a href="tokenizing-by-n-gram.html#cb69-3"></a><span class="st">  </span><span class="kw">filter</span>(word2 <span class="op">==</span><span class="st"> &quot;street&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb69-4"><a href="tokenizing-by-n-gram.html#cb69-4"></a><span class="st">  </span><span class="kw">count</span>(<span class="dt">street =</span> <span class="kw">str_c</span>(word1, word2, <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>), <span class="dt">sort =</span> <span class="ot">TRUE</span>)</span>
<span id="cb69-5"><a href="tokenizing-by-n-gram.html#cb69-5"></a><span class="co">#&gt; # A tibble: 28 x 2</span></span>
<span id="cb69-6"><a href="tokenizing-by-n-gram.html#cb69-6"></a><span class="co">#&gt;   street              n</span></span>
<span id="cb69-7"><a href="tokenizing-by-n-gram.html#cb69-7"></a><span class="co">#&gt;   &lt;chr&gt;           &lt;int&gt;</span></span>
<span id="cb69-8"><a href="tokenizing-by-n-gram.html#cb69-8"></a><span class="co">#&gt; 1 berkeley street    16</span></span>
<span id="cb69-9"><a href="tokenizing-by-n-gram.html#cb69-9"></a><span class="co">#&gt; 2 harley street      16</span></span>
<span id="cb69-10"><a href="tokenizing-by-n-gram.html#cb69-10"></a><span class="co">#&gt; 3 milsom street      16</span></span>
<span id="cb69-11"><a href="tokenizing-by-n-gram.html#cb69-11"></a><span class="co">#&gt; 4 pulteney street    15</span></span>
<span id="cb69-12"><a href="tokenizing-by-n-gram.html#cb69-12"></a><span class="co">#&gt; 5 wimpole street     10</span></span>
<span id="cb69-13"><a href="tokenizing-by-n-gram.html#cb69-13"></a><span class="co">#&gt; 6 bond street         9</span></span>
<span id="cb69-14"><a href="tokenizing-by-n-gram.html#cb69-14"></a><span class="co">#&gt; # ... with 22 more rows</span></span></code></pre></div>
<p>A bigram can also be treated as a term in a document in the same way that we treated individual words. For example, we can look at the weighted log odds (Section <a href="weighted-log-odds-ratio.html#weighted-log-odds-ratio">3.2</a>) of bigrams across Austen novels.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="tokenizing-by-n-gram.html#cb70-1"></a><span class="kw">library</span>(tidylo)</span>
<span id="cb70-2"><a href="tokenizing-by-n-gram.html#cb70-2"></a></span>
<span id="cb70-3"><a href="tokenizing-by-n-gram.html#cb70-3"></a>austen_bigrams <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-4"><a href="tokenizing-by-n-gram.html#cb70-4"></a><span class="st">  </span><span class="kw">count</span>(book, bigram, <span class="dt">sort =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-5"><a href="tokenizing-by-n-gram.html#cb70-5"></a><span class="st">  </span><span class="kw">bind_log_odds</span>(<span class="dt">set =</span> book, <span class="dt">feature =</span> bigram, <span class="dt">n =</span> n) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-6"><a href="tokenizing-by-n-gram.html#cb70-6"></a><span class="st">  </span><span class="kw">group_by</span>(book) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-7"><a href="tokenizing-by-n-gram.html#cb70-7"></a><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">15</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb70-8"><a href="tokenizing-by-n-gram.html#cb70-8"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></span>
<span id="cb70-9"><a href="tokenizing-by-n-gram.html#cb70-9"></a><span class="st">  </span><span class="kw">facet_bar</span>(<span class="dt">y =</span> bigram, <span class="dt">x =</span> log_odds, <span class="dt">by =</span> book, <span class="dt">nrow =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="relationship-between-words_files/figure-html/unnamed-chunk-6-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
<div id="using-bigrams-to-provide-context-in-sentiment-analysis" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Using bigrams to provide context in sentiment analysis</h3>
<p>Context matters in sentiment analysis. For example, the words “happy” and “like” will be counted as positive, even in a sentence like</p>
<blockquote>
<p>“I’m not happy and I don’t like it!”</p>
</blockquote>
<p>Now that we have the data organized into bigrams, it’s easy to tell how often words are preceded by a word like “not”:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="tokenizing-by-n-gram.html#cb71-1"></a>austen_separated <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb71-2"><a href="tokenizing-by-n-gram.html#cb71-2"></a><span class="st">  </span><span class="kw">filter</span>(word1 <span class="op">==</span><span class="st"> &quot;not&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb71-3"><a href="tokenizing-by-n-gram.html#cb71-3"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word2 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span></span>
<span id="cb71-4"><a href="tokenizing-by-n-gram.html#cb71-4"></a><span class="st">  </span><span class="kw">count</span>(word1, word2, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</span>
<span id="cb71-5"><a href="tokenizing-by-n-gram.html#cb71-5"></a><span class="co">#&gt; # A tibble: 988 x 3</span></span>
<span id="cb71-6"><a href="tokenizing-by-n-gram.html#cb71-6"></a><span class="co">#&gt;   word1 word2          n</span></span>
<span id="cb71-7"><a href="tokenizing-by-n-gram.html#cb71-7"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt;      &lt;int&gt;</span></span>
<span id="cb71-8"><a href="tokenizing-by-n-gram.html#cb71-8"></a><span class="co">#&gt; 1 not   hear          39</span></span>
<span id="cb71-9"><a href="tokenizing-by-n-gram.html#cb71-9"></a><span class="co">#&gt; 2 not   speak         35</span></span>
<span id="cb71-10"><a href="tokenizing-by-n-gram.html#cb71-10"></a><span class="co">#&gt; 3 not   expect        34</span></span>
<span id="cb71-11"><a href="tokenizing-by-n-gram.html#cb71-11"></a><span class="co">#&gt; 4 not   bear          33</span></span>
<span id="cb71-12"><a href="tokenizing-by-n-gram.html#cb71-12"></a><span class="co">#&gt; 5 not   imagine       26</span></span>
<span id="cb71-13"><a href="tokenizing-by-n-gram.html#cb71-13"></a><span class="co">#&gt; 6 not   understand    26</span></span>
<span id="cb71-14"><a href="tokenizing-by-n-gram.html#cb71-14"></a><span class="co">#&gt; # ... with 982 more rows</span></span></code></pre></div>
<p>Let’s use the AFINN lexicon for sentiment analysis, which you may recall gives a numeric sentiment value for each word, with positive or negative numbers indicating the direction of the sentiment.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="tokenizing-by-n-gram.html#cb72-1"></a>not_words &lt;-<span class="st"> </span>austen_separated <span class="op">%&gt;%</span></span>
<span id="cb72-2"><a href="tokenizing-by-n-gram.html#cb72-2"></a><span class="st">  </span><span class="kw">filter</span>(word1 <span class="op">==</span><span class="st"> &quot;not&quot;</span>) <span class="op">%&gt;%</span></span>
<span id="cb72-3"><a href="tokenizing-by-n-gram.html#cb72-3"></a><span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;afinn&quot;</span>), <span class="dt">by =</span> <span class="kw">c</span>(<span class="dt">word2 =</span> <span class="st">&quot;word&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb72-4"><a href="tokenizing-by-n-gram.html#cb72-4"></a><span class="st">  </span><span class="kw">count</span>(word1, word2, value, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</span>
<span id="cb72-5"><a href="tokenizing-by-n-gram.html#cb72-5"></a></span>
<span id="cb72-6"><a href="tokenizing-by-n-gram.html#cb72-6"></a>not_words</span>
<span id="cb72-7"><a href="tokenizing-by-n-gram.html#cb72-7"></a><span class="co">#&gt; # A tibble: 245 x 4</span></span>
<span id="cb72-8"><a href="tokenizing-by-n-gram.html#cb72-8"></a><span class="co">#&gt;   word1 word2 value     n</span></span>
<span id="cb72-9"><a href="tokenizing-by-n-gram.html#cb72-9"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;</span></span>
<span id="cb72-10"><a href="tokenizing-by-n-gram.html#cb72-10"></a><span class="co">#&gt; 1 not   like      2    99</span></span>
<span id="cb72-11"><a href="tokenizing-by-n-gram.html#cb72-11"></a><span class="co">#&gt; 2 not   help      2    82</span></span>
<span id="cb72-12"><a href="tokenizing-by-n-gram.html#cb72-12"></a><span class="co">#&gt; 3 not   want      1    45</span></span>
<span id="cb72-13"><a href="tokenizing-by-n-gram.html#cb72-13"></a><span class="co">#&gt; 4 not   wish      1    39</span></span>
<span id="cb72-14"><a href="tokenizing-by-n-gram.html#cb72-14"></a><span class="co">#&gt; 5 not   allow     1    36</span></span>
<span id="cb72-15"><a href="tokenizing-by-n-gram.html#cb72-15"></a><span class="co">#&gt; 6 not   care      2    23</span></span>
<span id="cb72-16"><a href="tokenizing-by-n-gram.html#cb72-16"></a><span class="co">#&gt; # ... with 239 more rows</span></span></code></pre></div>
<p>It’s worth asking which words contributed the most in the “wrong” direction. To compute that, we can multiply their value by the number of times they appear (so that a word with a value of +3 occurring 10 times has as much impact as a word with a sentiment value of +1 occurring 30 times). We visualize the result with a bar plot</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="tokenizing-by-n-gram.html#cb73-1"></a>not_words <span class="op">%&gt;%</span></span>
<span id="cb73-2"><a href="tokenizing-by-n-gram.html#cb73-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">contribution =</span> n <span class="op">*</span><span class="st"> </span>value,</span>
<span id="cb73-3"><a href="tokenizing-by-n-gram.html#cb73-3"></a>         <span class="dt">sign =</span> <span class="kw">if_else</span>(value <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;postive&quot;</span>, <span class="st">&quot;negative&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb73-4"><a href="tokenizing-by-n-gram.html#cb73-4"></a><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">20</span>, <span class="kw">abs</span>(contribution)) <span class="op">%&gt;%</span></span>
<span id="cb73-5"><a href="tokenizing-by-n-gram.html#cb73-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">word2 =</span> <span class="kw">fct_reorder</span>(word2, contribution)) <span class="op">%&gt;%</span></span>
<span id="cb73-6"><a href="tokenizing-by-n-gram.html#cb73-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> word2, <span class="dt">x =</span> contribution, <span class="dt">fill =</span> sign)) <span class="op">+</span></span>
<span id="cb73-7"><a href="tokenizing-by-n-gram.html#cb73-7"></a><span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span></span>
<span id="cb73-8"><a href="tokenizing-by-n-gram.html#cb73-8"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&#39;Words preceded by </span><span class="ch">\&quot;</span><span class="st">not</span><span class="ch">\&quot;</span><span class="st">&#39;</span>,</span>
<span id="cb73-9"><a href="tokenizing-by-n-gram.html#cb73-9"></a>       <span class="dt">x =</span> <span class="st">&quot;Sentiment value * number of occurrences&quot;</span>)</span></code></pre></div>
<p><img src="relationship-between-words_files/figure-html/unnamed-chunk-9-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>The bigrams “not like” and “not help” were overwhelmingly the largest causes of misidentification, making the text seem much more positive than it is. But we can see phrases like “not afraid” and “not fail” sometimes suggest text is more negative than it is.</p>
<p>“Not” isn’t the only term that provides some context for the following word. We could pick four common words <code>not</code>, <code>no</code>, <code>never</code> and <code>without</code> that negate the subsequent term, and use the same joining and counting approach to examine all of them at once.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="tokenizing-by-n-gram.html#cb74-1"></a>negation_words &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;not&quot;</span>, <span class="st">&quot;no&quot;</span>, <span class="st">&quot;never&quot;</span>, <span class="st">&quot;without&quot;</span>)</span>
<span id="cb74-2"><a href="tokenizing-by-n-gram.html#cb74-2"></a></span>
<span id="cb74-3"><a href="tokenizing-by-n-gram.html#cb74-3"></a>negated_words &lt;-<span class="st"> </span>austen_separated <span class="op">%&gt;%</span></span>
<span id="cb74-4"><a href="tokenizing-by-n-gram.html#cb74-4"></a><span class="st">  </span><span class="kw">filter</span>(word1 <span class="op">%in%</span><span class="st"> </span>negation_words) <span class="op">%&gt;%</span></span>
<span id="cb74-5"><a href="tokenizing-by-n-gram.html#cb74-5"></a><span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;afinn&quot;</span>), <span class="dt">by =</span> <span class="kw">c</span>(<span class="dt">word2 =</span> <span class="st">&quot;word&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb74-6"><a href="tokenizing-by-n-gram.html#cb74-6"></a><span class="st">  </span><span class="kw">count</span>(word1, word2, value, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</span>
<span id="cb74-7"><a href="tokenizing-by-n-gram.html#cb74-7"></a></span>
<span id="cb74-8"><a href="tokenizing-by-n-gram.html#cb74-8"></a>negated_words <span class="op">%&gt;%</span></span>
<span id="cb74-9"><a href="tokenizing-by-n-gram.html#cb74-9"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">contribution =</span> n <span class="op">*</span><span class="st"> </span>value,</span>
<span id="cb74-10"><a href="tokenizing-by-n-gram.html#cb74-10"></a>         <span class="dt">sign =</span> <span class="kw">if_else</span>(value <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;postive&quot;</span>, <span class="st">&quot;negative&quot;</span>)) <span class="op">%&gt;%</span></span>
<span id="cb74-11"><a href="tokenizing-by-n-gram.html#cb74-11"></a><span class="st">  </span><span class="kw">group_by</span>(word1) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb74-12"><a href="tokenizing-by-n-gram.html#cb74-12"></a><span class="st">  </span><span class="kw">top_n</span>(<span class="dv">20</span>, <span class="kw">abs</span>(contribution)) <span class="op">%&gt;%</span></span>
<span id="cb74-13"><a href="tokenizing-by-n-gram.html#cb74-13"></a><span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span></span>
<span id="cb74-14"><a href="tokenizing-by-n-gram.html#cb74-14"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> <span class="kw">reorder_within</span>(word2, contribution, word1), </span>
<span id="cb74-15"><a href="tokenizing-by-n-gram.html#cb74-15"></a>             <span class="dt">x =</span> contribution, </span>
<span id="cb74-16"><a href="tokenizing-by-n-gram.html#cb74-16"></a>             <span class="dt">fill =</span> sign)) <span class="op">+</span></span>
<span id="cb74-17"><a href="tokenizing-by-n-gram.html#cb74-17"></a><span class="st">  </span><span class="kw">geom_col</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb74-18"><a href="tokenizing-by-n-gram.html#cb74-18"></a><span class="st">  </span><span class="kw">scale_y_reordered</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb74-19"><a href="tokenizing-by-n-gram.html#cb74-19"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>word1, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb74-20"><a href="tokenizing-by-n-gram.html#cb74-20"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&#39;Words proceeded by a negation term&#39;</span>,</span>
<span id="cb74-21"><a href="tokenizing-by-n-gram.html#cb74-21"></a>       <span class="dt">x =</span> <span class="st">&quot;Sentiment value * number of occurrences&quot;</span>,</span>
<span id="cb74-22"><a href="tokenizing-by-n-gram.html#cb74-22"></a>       <span class="dt">title =</span> <span class="st">&quot;The most common positive or negative words to follow negations such as &#39;never&#39;, &#39;no&#39;, &#39;not&#39;, and &#39;without&#39;&quot;</span>)</span></code></pre></div>
<p><img src="relationship-between-words_files/figure-html/unnamed-chunk-10-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="visualizing-a-network-of-bigrams-with-ggraph" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Visualizing a network of bigrams with <code>ggraph</code></h3>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="tokenizing-by-n-gram.html#cb75-1"></a><span class="kw">library</span>(tidygraph)</span>
<span id="cb75-2"><a href="tokenizing-by-n-gram.html#cb75-2"></a><span class="kw">library</span>(ggraph)</span></code></pre></div>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="tokenizing-by-n-gram.html#cb76-1"></a>bigram_counts &lt;-<span class="st"> </span>austen_separated <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb76-2"><a href="tokenizing-by-n-gram.html#cb76-2"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word1 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word,</span>
<span id="cb76-3"><a href="tokenizing-by-n-gram.html#cb76-3"></a>         <span class="op">!</span>word2 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb76-4"><a href="tokenizing-by-n-gram.html#cb76-4"></a><span class="st">  </span><span class="kw">count</span>(word1, word2, <span class="dt">sort =</span> <span class="ot">TRUE</span>) </span></code></pre></div>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="tokenizing-by-n-gram.html#cb77-1"></a>bigram_graph &lt;-<span class="st"> </span>bigram_counts <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb77-2"><a href="tokenizing-by-n-gram.html#cb77-2"></a><span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">20</span>) <span class="op">%&gt;%</span></span>
<span id="cb77-3"><a href="tokenizing-by-n-gram.html#cb77-3"></a><span class="st">  </span><span class="kw">as_tbl_graph</span>()</span>
<span id="cb77-4"><a href="tokenizing-by-n-gram.html#cb77-4"></a></span>
<span id="cb77-5"><a href="tokenizing-by-n-gram.html#cb77-5"></a>bigram_graph</span>
<span id="cb77-6"><a href="tokenizing-by-n-gram.html#cb77-6"></a><span class="co">#&gt; # A tbl_graph: 91 nodes and 77 edges</span></span>
<span id="cb77-7"><a href="tokenizing-by-n-gram.html#cb77-7"></a><span class="co">#&gt; #</span></span>
<span id="cb77-8"><a href="tokenizing-by-n-gram.html#cb77-8"></a><span class="co">#&gt; # A directed acyclic simple graph with 17 components</span></span>
<span id="cb77-9"><a href="tokenizing-by-n-gram.html#cb77-9"></a><span class="co">#&gt; #</span></span>
<span id="cb77-10"><a href="tokenizing-by-n-gram.html#cb77-10"></a><span class="co">#&gt; # Node Data: 91 x 1 (active)</span></span>
<span id="cb77-11"><a href="tokenizing-by-n-gram.html#cb77-11"></a><span class="co">#&gt;   name   </span></span>
<span id="cb77-12"><a href="tokenizing-by-n-gram.html#cb77-12"></a><span class="co">#&gt;   &lt;chr&gt;  </span></span>
<span id="cb77-13"><a href="tokenizing-by-n-gram.html#cb77-13"></a><span class="co">#&gt; 1 sir    </span></span>
<span id="cb77-14"><a href="tokenizing-by-n-gram.html#cb77-14"></a><span class="co">#&gt; 2 miss   </span></span>
<span id="cb77-15"><a href="tokenizing-by-n-gram.html#cb77-15"></a><span class="co">#&gt; 3 captain</span></span>
<span id="cb77-16"><a href="tokenizing-by-n-gram.html#cb77-16"></a><span class="co">#&gt; 4 frank  </span></span>
<span id="cb77-17"><a href="tokenizing-by-n-gram.html#cb77-17"></a><span class="co">#&gt; 5 lady   </span></span>
<span id="cb77-18"><a href="tokenizing-by-n-gram.html#cb77-18"></a><span class="co">#&gt; 6 colonel</span></span>
<span id="cb77-19"><a href="tokenizing-by-n-gram.html#cb77-19"></a><span class="co">#&gt; # ... with 85 more rows</span></span>
<span id="cb77-20"><a href="tokenizing-by-n-gram.html#cb77-20"></a><span class="co">#&gt; #</span></span>
<span id="cb77-21"><a href="tokenizing-by-n-gram.html#cb77-21"></a><span class="co">#&gt; # Edge Data: 77 x 3</span></span>
<span id="cb77-22"><a href="tokenizing-by-n-gram.html#cb77-22"></a><span class="co">#&gt;    from    to     n</span></span>
<span id="cb77-23"><a href="tokenizing-by-n-gram.html#cb77-23"></a><span class="co">#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;</span></span>
<span id="cb77-24"><a href="tokenizing-by-n-gram.html#cb77-24"></a><span class="co">#&gt; 1     1    28   287</span></span>
<span id="cb77-25"><a href="tokenizing-by-n-gram.html#cb77-25"></a><span class="co">#&gt; 2     2    29   215</span></span>
<span id="cb77-26"><a href="tokenizing-by-n-gram.html#cb77-26"></a><span class="co">#&gt; 3     3    30   170</span></span>
<span id="cb77-27"><a href="tokenizing-by-n-gram.html#cb77-27"></a><span class="co">#&gt; # ... with 74 more rows</span></span></code></pre></div>
<p>Note how <code>tidygraph</code> handles network data, the main <code>tbl_graph</code> object splits a network into two data frames: <strong>Node data</strong> and <strong>Edge data</strong></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="tokenizing-by-n-gram.html#cb78-1"></a><span class="kw">ggraph</span>(bigram_graph, <span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb78-2"><a href="tokenizing-by-n-gram.html#cb78-2"></a><span class="st">  </span><span class="kw">geom_edge_link</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb78-3"><a href="tokenizing-by-n-gram.html#cb78-3"></a><span class="st">  </span><span class="kw">geom_node_point</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb78-4"><a href="tokenizing-by-n-gram.html#cb78-4"></a><span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">vjust =</span> <span class="dv">1</span>, <span class="dt">hjust =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="relationship-between-words_files/figure-html/unnamed-chunk-14-1.png" width="1152" /></p>
<p>We see that salutations such as “miss”, “lady”, “sir”, “and”colonel" are common centers of nodes, which are often followed by names. We also see pairs or triplets along the outside that form common short phrases (“half hour”, “thousand pounds”, or “short time/pause”).</p>
<p>Note that this is a visualization of a Markov chain, a common model in text processing, where the choice of a word only depends on its previous word. In this case, a random generator following this model might spit out “dear”, then “sir”, then “william/walter/thomas/thomas’s”, by following each word to the most common words that follow it.</p>
<p>A polished graph:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="tokenizing-by-n-gram.html#cb79-1"></a>arrow &lt;-<span class="st"> </span>grid<span class="op">::</span><span class="kw">arrow</span>(<span class="dt">type =</span> <span class="st">&quot;closed&quot;</span>, <span class="dt">length =</span> <span class="kw">unit</span>(.<span class="dv">15</span>, <span class="st">&quot;inches&quot;</span>))</span>
<span id="cb79-2"><a href="tokenizing-by-n-gram.html#cb79-2"></a></span>
<span id="cb79-3"><a href="tokenizing-by-n-gram.html#cb79-3"></a><span class="kw">ggraph</span>(bigram_graph, <span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb79-4"><a href="tokenizing-by-n-gram.html#cb79-4"></a><span class="st">  </span><span class="kw">geom_edge_link</span>(<span class="kw">aes</span>(<span class="dt">alpha =</span> n), <span class="dt">show.legend =</span> F, </span>
<span id="cb79-5"><a href="tokenizing-by-n-gram.html#cb79-5"></a>                 <span class="dt">arrow =</span> arrow, <span class="dt">end_cap =</span> <span class="kw">circle</span>(<span class="fl">0.07</span>, <span class="st">&quot;inches&quot;</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb79-6"><a href="tokenizing-by-n-gram.html#cb79-6"></a><span class="st">  </span><span class="kw">geom_node_point</span>(<span class="dt">color =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="dt">size =</span> <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb79-7"><a href="tokenizing-by-n-gram.html#cb79-7"></a><span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">vjust =</span> <span class="dv">1</span>, <span class="dt">hjust =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="relationship-between-words_files/figure-html/unnamed-chunk-15-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
<div id="visualizing-friends" class="section level3">
<h3><span class="header-section-number">4.1.5</span> Visualizing “friends”</h3>
<p>Here I deviate from the original text, where Julia and David analyzed King James Version of the Bible. However, I have collected the transcripts of the famous TV series, friends (season 1). Let’s start a simple analysis first by loading the data</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="tokenizing-by-n-gram.html#cb80-1"></a>friends &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/friends_season_1.csv&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb80-2"><a href="tokenizing-by-n-gram.html#cb80-2"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>type)</span>
<span id="cb80-3"><a href="tokenizing-by-n-gram.html#cb80-3"></a><span class="kw">glimpse</span>(friends)</span>
<span id="cb80-4"><a href="tokenizing-by-n-gram.html#cb80-4"></a><span class="co">#&gt; Observations: 5,974</span></span>
<span id="cb80-5"><a href="tokenizing-by-n-gram.html#cb80-5"></a><span class="co">#&gt; Variables: 6</span></span>
<span id="cb80-6"><a href="tokenizing-by-n-gram.html#cb80-6"></a><span class="co">#&gt; $ episode &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</span></span>
<span id="cb80-7"><a href="tokenizing-by-n-gram.html#cb80-7"></a><span class="co">#&gt; $ person  &lt;chr&gt; &quot;monica&quot;, &quot;joey&quot;, &quot;chandler&quot;, &quot;phoebe&quot;, &quot;phoebe&quot;, &quot;monica&quot;,...</span></span>
<span id="cb80-8"><a href="tokenizing-by-n-gram.html#cb80-8"></a><span class="co">#&gt; $ line    &lt;chr&gt; &quot;there&#39;s nothing to tell! he&#39;s just some guyi work with!&quot;, ...</span></span>
<span id="cb80-9"><a href="tokenizing-by-n-gram.html#cb80-9"></a><span class="co">#&gt; $ id      &lt;chr&gt; &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;0101&quot;, &quot;01...</span></span>
<span id="cb80-10"><a href="tokenizing-by-n-gram.html#cb80-10"></a><span class="co">#&gt; $ title   &lt;chr&gt; &quot;The One Where Monica Gets a New Roomate (The Pilot-The Unc...</span></span>
<span id="cb80-11"><a href="tokenizing-by-n-gram.html#cb80-11"></a><span class="co">#&gt; $ season  &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;,...</span></span></code></pre></div>
<p>Retrieve a clean data frame with word counts (bigrams did not work very well)</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="tokenizing-by-n-gram.html#cb81-1"></a>friends_bigram &lt;-<span class="st"> </span>friends <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb81-2"><a href="tokenizing-by-n-gram.html#cb81-2"></a><span class="st">  </span><span class="kw">unnest_tokens</span>(word, line, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span></span>
<span id="cb81-3"><a href="tokenizing-by-n-gram.html#cb81-3"></a><span class="st">  </span><span class="kw">separate</span>(word, <span class="dt">into =</span> <span class="kw">c</span>(<span class="st">&quot;word1&quot;</span>, <span class="st">&quot;word2&quot;</span>), <span class="dt">sep =</span> <span class="st">&quot; &quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb81-4"><a href="tokenizing-by-n-gram.html#cb81-4"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span>word1 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word,</span>
<span id="cb81-5"><a href="tokenizing-by-n-gram.html#cb81-5"></a>         <span class="op">!</span>word2 <span class="op">%in%</span><span class="st"> </span>stop_words<span class="op">$</span>word) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb81-6"><a href="tokenizing-by-n-gram.html#cb81-6"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="st"> </span>(<span class="kw">is.na</span>(word1) <span class="op">|</span><span class="st"> </span><span class="kw">is.na</span>(word2)))</span>
<span id="cb81-7"><a href="tokenizing-by-n-gram.html#cb81-7"></a></span>
<span id="cb81-8"><a href="tokenizing-by-n-gram.html#cb81-8"></a>friends_count &lt;-<span class="st"> </span>friends_bigram <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb81-9"><a href="tokenizing-by-n-gram.html#cb81-9"></a><span class="st">  </span><span class="kw">count</span>(word1, word2, <span class="dt">sort =</span> <span class="ot">TRUE</span>)</span>
<span id="cb81-10"><a href="tokenizing-by-n-gram.html#cb81-10"></a></span>
<span id="cb81-11"><a href="tokenizing-by-n-gram.html#cb81-11"></a>friends_count</span>
<span id="cb81-12"><a href="tokenizing-by-n-gram.html#cb81-12"></a><span class="co">#&gt; # A tibble: 5,466 x 3</span></span>
<span id="cb81-13"><a href="tokenizing-by-n-gram.html#cb81-13"></a><span class="co">#&gt;   word1 word2     n</span></span>
<span id="cb81-14"><a href="tokenizing-by-n-gram.html#cb81-14"></a><span class="co">#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;int&gt;</span></span>
<span id="cb81-15"><a href="tokenizing-by-n-gram.html#cb81-15"></a><span class="co">#&gt; 1 hey   hey      36</span></span>
<span id="cb81-16"><a href="tokenizing-by-n-gram.html#cb81-16"></a><span class="co">#&gt; 2 ow    ow       29</span></span>
<span id="cb81-17"><a href="tokenizing-by-n-gram.html#cb81-17"></a><span class="co">#&gt; 3 la    la       27</span></span>
<span id="cb81-18"><a href="tokenizing-by-n-gram.html#cb81-18"></a><span class="co">#&gt; 4 yeah  yeah     27</span></span>
<span id="cb81-19"><a href="tokenizing-by-n-gram.html#cb81-19"></a><span class="co">#&gt; 5 wait  wait     17</span></span>
<span id="cb81-20"><a href="tokenizing-by-n-gram.html#cb81-20"></a><span class="co">#&gt; 6 uh    huh      16</span></span>
<span id="cb81-21"><a href="tokenizing-by-n-gram.html#cb81-21"></a><span class="co">#&gt; # ... with 5,460 more rows</span></span></code></pre></div>
<p>Draw a network</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="tokenizing-by-n-gram.html#cb82-1"></a>friends_count <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb82-2"><a href="tokenizing-by-n-gram.html#cb82-2"></a><span class="st">  </span><span class="kw">filter</span>(n <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span>) <span class="op">%&gt;%</span></span>
<span id="cb82-3"><a href="tokenizing-by-n-gram.html#cb82-3"></a><span class="st">  </span><span class="kw">as_tbl_graph</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb82-4"><a href="tokenizing-by-n-gram.html#cb82-4"></a><span class="st">  </span><span class="kw">ggraph</span>(<span class="dt">layout =</span> <span class="st">&quot;fr&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb82-5"><a href="tokenizing-by-n-gram.html#cb82-5"></a><span class="st">  </span><span class="kw">geom_edge_link</span>(<span class="kw">aes</span>(<span class="dt">alpha =</span> n), <span class="dt">show.legend =</span> <span class="ot">FALSE</span>,</span>
<span id="cb82-6"><a href="tokenizing-by-n-gram.html#cb82-6"></a>                 <span class="dt">arrow =</span> arrow, <span class="dt">end_cap =</span> <span class="kw">circle</span>(<span class="fl">0.07</span>, <span class="st">&quot;inches&quot;</span>)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb82-7"><a href="tokenizing-by-n-gram.html#cb82-7"></a><span class="st">  </span><span class="kw">geom_node_point</span>(<span class="dt">color =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="dt">size =</span> <span class="dv">5</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb82-8"><a href="tokenizing-by-n-gram.html#cb82-8"></a><span class="st">  </span><span class="kw">geom_node_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> name), <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">vjust =</span> <span class="dv">1</span>, <span class="dt">hjust =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="relationship-between-words_files/figure-html/unnamed-chunk-18-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="relationships-between-words-n-grams-and-correlations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="counting-and-correlating-pairs-of-words-with-widyr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/tidy-text-mining/edit/master/book/relationship-between-words.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
